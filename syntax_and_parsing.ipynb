{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Free Grammers - CFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-One of the most commen types of formal grammar used in NLP.\n",
    "\n",
    "-CFGs define rules where a single non-terminal on the left-hand side of a production rule can be replaced by a sequence of terminals and non-terminals.\n",
    "\n",
    "(Terminals : The actual words or symbols of the language.)\n",
    "\n",
    "(Non-terminals : Abstract categories (like noun phrase or verb phrase) used to group terminals and describe patterns in the language.)\n",
    "\n",
    "S      ->       NP VP\n",
    "\n",
    "NP     ->       Det N\n",
    "\n",
    "VP     ->       V  NP\n",
    "\n",
    "Det    ->       'the' | 'a'\n",
    "\n",
    "N      ->       'cat' | 'dog'\n",
    "\n",
    "V      ->       'chases' | 'sees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (Det the) (N cat)) (VP (V chases) (NP (Det the) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  VP -> V NP\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'cat' | 'dog'\n",
    "  V -> 'chases' | 'sees'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "sentence = \"the cat chases the dog\".split()\n",
    "\n",
    "for i in parser.parse(sentence):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'chased', 'the', 'cat'],\n",
       " ['the', 'cat', 'chased', 'the', 'dog'],\n",
       " ['the', 'cat', 'chased', 'the', 'ball'],\n",
       " ['the', 'cat', 'chased', 'a', 'cat'],\n",
       " ['the', 'cat', 'chased', 'a', 'dog'],\n",
       " ['the', 'cat', 'chased', 'a', 'ball'],\n",
       " ['the', 'cat', 'saw', 'the', 'cat'],\n",
       " ['the', 'cat', 'saw', 'the', 'dog'],\n",
       " ['the', 'cat', 'saw', 'the', 'ball'],\n",
       " ['the', 'cat', 'saw', 'a', 'cat'],\n",
       " ['the', 'cat', 'saw', 'a', 'dog'],\n",
       " ['the', 'cat', 'saw', 'a', 'ball'],\n",
       " ['the', 'cat', 'chased'],\n",
       " ['the', 'cat', 'saw'],\n",
       " ['the', 'dog', 'chased', 'the', 'cat'],\n",
       " ['the', 'dog', 'chased', 'the', 'dog'],\n",
       " ['the', 'dog', 'chased', 'the', 'ball'],\n",
       " ['the', 'dog', 'chased', 'a', 'cat'],\n",
       " ['the', 'dog', 'chased', 'a', 'dog'],\n",
       " ['the', 'dog', 'chased', 'a', 'ball'],\n",
       " ['the', 'dog', 'saw', 'the', 'cat'],\n",
       " ['the', 'dog', 'saw', 'the', 'dog'],\n",
       " ['the', 'dog', 'saw', 'the', 'ball'],\n",
       " ['the', 'dog', 'saw', 'a', 'cat'],\n",
       " ['the', 'dog', 'saw', 'a', 'dog'],\n",
       " ['the', 'dog', 'saw', 'a', 'ball'],\n",
       " ['the', 'dog', 'chased'],\n",
       " ['the', 'dog', 'saw'],\n",
       " ['the', 'ball', 'chased', 'the', 'cat'],\n",
       " ['the', 'ball', 'chased', 'the', 'dog'],\n",
       " ['the', 'ball', 'chased', 'the', 'ball'],\n",
       " ['the', 'ball', 'chased', 'a', 'cat'],\n",
       " ['the', 'ball', 'chased', 'a', 'dog'],\n",
       " ['the', 'ball', 'chased', 'a', 'ball'],\n",
       " ['the', 'ball', 'saw', 'the', 'cat'],\n",
       " ['the', 'ball', 'saw', 'the', 'dog'],\n",
       " ['the', 'ball', 'saw', 'the', 'ball'],\n",
       " ['the', 'ball', 'saw', 'a', 'cat'],\n",
       " ['the', 'ball', 'saw', 'a', 'dog'],\n",
       " ['the', 'ball', 'saw', 'a', 'ball'],\n",
       " ['the', 'ball', 'chased'],\n",
       " ['the', 'ball', 'saw'],\n",
       " ['a', 'cat', 'chased', 'the', 'cat'],\n",
       " ['a', 'cat', 'chased', 'the', 'dog'],\n",
       " ['a', 'cat', 'chased', 'the', 'ball'],\n",
       " ['a', 'cat', 'chased', 'a', 'cat'],\n",
       " ['a', 'cat', 'chased', 'a', 'dog'],\n",
       " ['a', 'cat', 'chased', 'a', 'ball'],\n",
       " ['a', 'cat', 'saw', 'the', 'cat'],\n",
       " ['a', 'cat', 'saw', 'the', 'dog'],\n",
       " ['a', 'cat', 'saw', 'the', 'ball'],\n",
       " ['a', 'cat', 'saw', 'a', 'cat'],\n",
       " ['a', 'cat', 'saw', 'a', 'dog'],\n",
       " ['a', 'cat', 'saw', 'a', 'ball'],\n",
       " ['a', 'cat', 'chased'],\n",
       " ['a', 'cat', 'saw'],\n",
       " ['a', 'dog', 'chased', 'the', 'cat'],\n",
       " ['a', 'dog', 'chased', 'the', 'dog'],\n",
       " ['a', 'dog', 'chased', 'the', 'ball'],\n",
       " ['a', 'dog', 'chased', 'a', 'cat'],\n",
       " ['a', 'dog', 'chased', 'a', 'dog'],\n",
       " ['a', 'dog', 'chased', 'a', 'ball'],\n",
       " ['a', 'dog', 'saw', 'the', 'cat'],\n",
       " ['a', 'dog', 'saw', 'the', 'dog'],\n",
       " ['a', 'dog', 'saw', 'the', 'ball'],\n",
       " ['a', 'dog', 'saw', 'a', 'cat'],\n",
       " ['a', 'dog', 'saw', 'a', 'dog'],\n",
       " ['a', 'dog', 'saw', 'a', 'ball'],\n",
       " ['a', 'dog', 'chased'],\n",
       " ['a', 'dog', 'saw'],\n",
       " ['a', 'ball', 'chased', 'the', 'cat'],\n",
       " ['a', 'ball', 'chased', 'the', 'dog'],\n",
       " ['a', 'ball', 'chased', 'the', 'ball'],\n",
       " ['a', 'ball', 'chased', 'a', 'cat'],\n",
       " ['a', 'ball', 'chased', 'a', 'dog'],\n",
       " ['a', 'ball', 'chased', 'a', 'ball'],\n",
       " ['a', 'ball', 'saw', 'the', 'cat'],\n",
       " ['a', 'ball', 'saw', 'the', 'dog'],\n",
       " ['a', 'ball', 'saw', 'the', 'ball'],\n",
       " ['a', 'ball', 'saw', 'a', 'cat'],\n",
       " ['a', 'ball', 'saw', 'a', 'dog'],\n",
       " ['a', 'ball', 'saw', 'a', 'ball'],\n",
       " ['a', 'ball', 'chased'],\n",
       " ['a', 'ball', 'saw']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "cfg = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N\n",
    "    VP -> V NP | V\n",
    "    Det -> 'the' | 'a'\n",
    "    N -> 'cat' | 'dog' | 'ball'\n",
    "    V -> 'chased' | 'saw'\n",
    "\"\"\")\n",
    "\n",
    "#a function that generates all possible sentences of a given CFG.\n",
    "\n",
    "def sentence_generate(grammar, starter):\n",
    "\n",
    "    productions = grammar.productions(lhs=starter)\n",
    "\n",
    "    for production in productions:\n",
    "        if all(isinstance(symbol, str) for symbol in production.rhs()):\n",
    "            yield list(production.rhs())\n",
    "        else:\n",
    "            for sub_sentence in combine_symbols(grammar, production.rhs()):\n",
    "                yield sub_sentence\n",
    "\n",
    "def combine_symbols(grammar,symbols):\n",
    "\n",
    "    if not symbols:\n",
    "        yield []\n",
    "    else:\n",
    "        first, rest = symbols[0], symbols[1:]\n",
    "        if isinstance(first, nltk.grammar.Nonterminal):\n",
    "            for first_part in sentence_generate(grammar, first):\n",
    "                for rest_part in combine_symbols(grammar, rest):\n",
    "                    yield first_part + rest_part  \n",
    "        else:\n",
    "            for rest_part in combine_symbols(grammar, rest):\n",
    "                yield [first] + rest_part  \n",
    "\n",
    "example_sentences = list(sentence_generate(cfg, nltk.Nonterminal('S')))\n",
    "example_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (5.28.3)\n",
      "Requirement already satisfied: requests in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (3.2.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\emre-\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Constituency Parsing using Stanza (these approaches ensure that sentences conform to the\n",
    "#grammatical rules of the language, which can lead to more accurate language understanding \n",
    "#and generation.)\n",
    "\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 17:30:01 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-12-12 17:30:01 INFO: Using device: cpu\n",
      "2024-12-12 17:30:01 INFO: Loading: tokenize\n",
      "2024-12-12 17:30:02 INFO: Loading: mwt\n",
      "2024-12-12 17:30:02 INFO: Loading: pos\n",
      "2024-12-12 17:30:02 INFO: Loading: lemma\n",
      "2024-12-12 17:30:02 INFO: Loading: constituency\n",
      "2024-12-12 17:30:02 INFO: Loading: depparse\n",
      "2024-12-12 17:30:03 INFO: Loading: sentiment\n",
      "2024-12-12 17:30:03 INFO: Loading: ner\n",
      "2024-12-12 17:30:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (DT The) (NN cat)) (VP (VBZ chases) (NP (DT the) (NN dog))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp_stanza = stanza.Pipeline('en', processor='tokenize,pos,constituency', download_method=None)\n",
    "doc_stanza = nlp_stanza(\"The cat chases the dog.\")\n",
    "for sentence in doc_stanza.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROOT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = doc_stanza.sentences[0].constituency\n",
    "tree.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((S (NP (DT The) (NN cat)) (VP (VBZ chases) (NP (DT the) (NN dog))) (. .)),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((NP (DT The) (NN cat)), (VP (VBZ chases) (NP (DT the) (NN dog))), (. .))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.children[0].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 17:31:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67b707383ba4b3280b94a9e2d7dfad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 17:31:16 INFO: Downloaded file to C:\\Users\\emre-\\stanza_resources\\resources.json\n",
      "2024-12-12 17:31:17 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-12-12 17:31:17 INFO: Using device: cpu\n",
      "2024-12-12 17:31:17 INFO: Loading: tokenize\n",
      "2024-12-12 17:31:17 INFO: Loading: mwt\n",
      "2024-12-12 17:31:17 INFO: Loading: pos\n",
      "2024-12-12 17:31:17 INFO: Loading: lemma\n",
      "2024-12-12 17:31:17 INFO: Loading: depparse\n",
      "2024-12-12 17:31:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Dependency Parsing using Stanza\n",
    "import stanza\n",
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: The\thead id: 2\thead: cat\tdeprel: det\n",
      "id: 2\tword: cat\thead id: 3\thead: chases\tdeprel: nsubj\n",
      "id: 3\tword: chases\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: the\thead id: 5\thead: dog\tdeprel: det\n",
      "id: 5\tword: dog\thead id: 3\thead: chases\tdeprel: obj\n",
      "id: 6\tword: .\thead id: 3\thead: chases\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "doc_stanza = nlp_stanza('The cat chases the dog.')\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc_stanza.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> 2, det\n",
      "cat -> 3, nsubj\n",
      "chases -> 0, root\n",
      "the -> 5, det\n",
      "dog -> 3, obj\n",
      ". -> 3, punct\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc_stanza.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f\"{word.text} -> {word.head}, {word.deprel}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
