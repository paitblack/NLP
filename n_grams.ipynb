{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sentences;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am going to meet my friends', 'I am going to go to the store', 'I am going to have lunch', 'I am going to take a nap', 'I am going to watch TV', 'I am going to play video games', 'I am going to go for a walk', 'I am going to read a book', 'I am going to call my parents', 'I am going to write an email', 'I am going to clean my room', 'I am going to do my laundry', 'I am going to cook dinner', 'I am going to go to bed']\n"
     ]
    }
   ],
   "source": [
    "continuations = [\n",
    "    \"meet my friends\",\n",
    "    \"go to the store\",\n",
    "    \"have lunch\",\n",
    "    \"take a nap\",\n",
    "    \"watch TV\",\n",
    "    \"play video games\",\n",
    "    \"go for a walk\",\n",
    "    \"read a book\",\n",
    "    \"call my parents\",\n",
    "    \"write an email\",\n",
    "    \"clean my room\",\n",
    "    \"do my laundry\",\n",
    "    \"cook dinner\",\n",
    "    \"go to bed\",\n",
    "]\n",
    "\n",
    "sentences = [\"I am going to \" + i for i in continuations]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'going', 'to', 'meet', 'my', 'friends', 'i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'i', 'am', 'going', 'to', 'have', 'lunch', 'i', 'am', 'going', 'to', 'take', 'a', 'nap', 'i', 'am', 'going', 'to', 'watch', 'tv', 'i', 'am', 'going', 'to', 'play', 'video', 'games', 'i', 'am', 'going', 'to', 'go', 'for', 'a', 'walk', 'i', 'am', 'going', 'to', 'read', 'a', 'book', 'i', 'am', 'going', 'to', 'call', 'my', 'parents', 'i', 'am', 'going', 'to', 'write', 'an', 'email', 'i', 'am', 'going', 'to', 'clean', 'my', 'room', 'i', 'am', 'going', 'to', 'do', 'my', 'laundry', 'i', 'am', 'going', 'to', 'cook', 'dinner', 'i', 'am', 'going', 'to', 'go', 'to', 'bed']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences = []\n",
    "\n",
    "for i in sentences:\n",
    "    tokens = [a.lower() for a in word_tokenize(i)]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "tokenized_list = [x for X in tokenized_sentences for x in X]\n",
    "print(tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency List;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 16), ('i', 14), ('am', 14), ('going', 14), ('my', 4), ('a', 3), ('go', 3), ('games', 1), ('lunch', 1), ('friends', 1), ('video', 1), ('meet', 1), ('laundry', 1), ('play', 1), ('tv', 1), ('room', 1), ('for', 1), ('take', 1), ('write', 1), ('bed', 1), ('an', 1), ('store', 1), ('dinner', 1), ('have', 1), ('call', 1), ('nap', 1), ('book', 1), ('do', 1), ('read', 1), ('walk', 1), ('email', 1), ('parents', 1), ('the', 1), ('clean', 1), ('watch', 1), ('cook', 1)]\n"
     ]
    }
   ],
   "source": [
    "V = set(tokenized_list)\n",
    "\n",
    "freq_dict = {v:0 for v in V}\n",
    "\n",
    "for i in tokenized_list:\n",
    "    freq_dict[i]+=1\n",
    "\n",
    "uniqram_freqs = [(x,freq_dict[x]) for x in freq_dict]\n",
    "uniqram_freqs.sort(key = lambda x : x[1] ,reverse=True)\n",
    "\n",
    "print(uniqram_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 0.16494845360824742), ('i', 0.14432989690721648), ('am', 0.14432989690721648), ('going', 0.14432989690721648), ('my', 0.041237113402061855), ('a', 0.030927835051546393), ('go', 0.030927835051546393), ('games', 0.010309278350515464), ('lunch', 0.010309278350515464), ('friends', 0.010309278350515464), ('video', 0.010309278350515464), ('meet', 0.010309278350515464), ('laundry', 0.010309278350515464), ('play', 0.010309278350515464), ('tv', 0.010309278350515464), ('room', 0.010309278350515464), ('for', 0.010309278350515464), ('take', 0.010309278350515464), ('write', 0.010309278350515464), ('bed', 0.010309278350515464), ('an', 0.010309278350515464), ('store', 0.010309278350515464), ('dinner', 0.010309278350515464), ('have', 0.010309278350515464), ('call', 0.010309278350515464), ('nap', 0.010309278350515464), ('book', 0.010309278350515464), ('do', 0.010309278350515464), ('read', 0.010309278350515464), ('walk', 0.010309278350515464), ('email', 0.010309278350515464), ('parents', 0.010309278350515464), ('the', 0.010309278350515464), ('clean', 0.010309278350515464), ('watch', 0.010309278350515464), ('cook', 0.010309278350515464)]\n"
     ]
    }
   ],
   "source": [
    "#P(w_i)=c(w_i)/N\n",
    "\n",
    "N = len(tokenized_list)\n",
    "probabilities = [(x,freq_dict[x]/N) for x in freq_dict]\n",
    "probabilities.sort(key=lambda x : x[1], reverse=True)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lenght :  8\n",
      "Min lenght :  6\n",
      "How much sentences we have?  14\n",
      "1) to am nap a i i i \n",
      "2) i to going going going bed to \n",
      "3) going to to i an i a \n",
      "4) my dinner to i to to \n",
      "5) i going room cook laundry a \n",
      "6) going going going i to to \n",
      "7) i am i take going going to \n",
      "8) cook going lunch going book am going \n",
      "9) bed cook am am write parents \n",
      "10) to am friends am meet store \n",
      "11) my call am the going am \n",
      "12) to am to am play games a \n",
      "13) am my i going i to \n",
      "14) i have to to a go going \n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "splitted_sentences = [x.split() for x in sentences]\n",
    "lenghts = [len(x) for x in splitted_sentences]\n",
    "lenghts.sort()\n",
    "max_length = lenghts[-1]\n",
    "min_length = lenghts[0]\n",
    "\n",
    "print(\"Max lenght : \",max_length)\n",
    "print(\"Min lenght : \",min_length)\n",
    "print(\"How much sentences we have? \", len(lenghts))\n",
    "\n",
    "sample_space = [[x]*freq_dict[x] for x in freq_dict]\n",
    "sample_space = [x for X in sample_space for x in X]\n",
    "\n",
    "for i in range(14):\n",
    "    generated_sentence = \"\"\n",
    "    for k in range(rnd.randrange(min_length,max_length)):\n",
    "        generated_sentence += rnd.choice(sample_space) + \" \"\n",
    "    print(f\"{i+1})\",generated_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P( w2 | w1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'i', 'am', 'going', 'to', 'meet', 'my', 'friends', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'have', 'lunch', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'take', 'a', 'nap', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'watch', 'tv', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'play', 'video', 'games', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'for', 'a', 'walk', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'read', 'a', 'book', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'call', 'my', 'parents', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'write', 'an', 'email', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'clean', 'my', 'room', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'do', 'my', 'laundry', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'cook', 'dinner', '<eos>']\n",
      "['<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'bed', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = [\"<bos>\"] + [token.lower() for token in word_tokenize(sentence)] + [\"<eos>\"]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "token_list = [x for X in tokenized_sentences for x in X]\n",
    "print(*tokenized_sentences, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<bos>', 'i'), 14)\n",
      "(('i', 'am'), 14)\n",
      "(('am', 'going'), 14)\n",
      "(('going', 'to'), 14)\n",
      "(('to', 'go'), 3)\n",
      "(('go', 'to'), 2)\n",
      "(('to', 'meet'), 1)\n",
      "(('meet', 'my'), 1)\n",
      "(('my', 'friends'), 1)\n",
      "(('friends', '<eos>'), 1)\n",
      "(('to', 'the'), 1)\n",
      "(('the', 'store'), 1)\n",
      "(('store', '<eos>'), 1)\n",
      "(('to', 'have'), 1)\n",
      "(('have', 'lunch'), 1)\n",
      "(('lunch', '<eos>'), 1)\n",
      "(('to', 'take'), 1)\n",
      "(('take', 'a'), 1)\n",
      "(('a', 'nap'), 1)\n",
      "(('nap', '<eos>'), 1)\n",
      "(('to', 'watch'), 1)\n",
      "(('watch', 'tv'), 1)\n",
      "(('tv', '<eos>'), 1)\n",
      "(('to', 'play'), 1)\n",
      "(('play', 'video'), 1)\n",
      "(('video', 'games'), 1)\n",
      "(('games', '<eos>'), 1)\n",
      "(('go', 'for'), 1)\n",
      "(('for', 'a'), 1)\n",
      "(('a', 'walk'), 1)\n",
      "(('walk', '<eos>'), 1)\n",
      "(('to', 'read'), 1)\n",
      "(('read', 'a'), 1)\n",
      "(('a', 'book'), 1)\n",
      "(('book', '<eos>'), 1)\n",
      "(('to', 'call'), 1)\n",
      "(('call', 'my'), 1)\n",
      "(('my', 'parents'), 1)\n",
      "(('parents', '<eos>'), 1)\n",
      "(('to', 'write'), 1)\n",
      "(('write', 'an'), 1)\n",
      "(('an', 'email'), 1)\n",
      "(('email', '<eos>'), 1)\n",
      "(('to', 'clean'), 1)\n",
      "(('clean', 'my'), 1)\n",
      "(('my', 'room'), 1)\n",
      "(('room', '<eos>'), 1)\n",
      "(('to', 'do'), 1)\n",
      "(('do', 'my'), 1)\n",
      "(('my', 'laundry'), 1)\n",
      "(('laundry', '<eos>'), 1)\n",
      "(('to', 'cook'), 1)\n",
      "(('cook', 'dinner'), 1)\n",
      "(('dinner', '<eos>'), 1)\n",
      "(('to', 'bed'), 1)\n",
      "(('bed', '<eos>'), 1)\n"
     ]
    }
   ],
   "source": [
    "#2-grams freq list\n",
    "\n",
    "bigrams = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in range(len(sentence)-1):\n",
    "        bigrams.append((sentence[word],sentence[word+1]))\n",
    "\n",
    "freqs_dicts_of_bigrams = {x:0 for x in bigrams}\n",
    "\n",
    "for i in bigrams:\n",
    "    freqs_dicts_of_bigrams[i] += 1\n",
    "\n",
    "freqs_list_of_bigrams = [(x, freqs_dicts_of_bigrams[x]) for x in freqs_dicts_of_bigrams]\n",
    "freqs_list_of_bigrams.sort(key = lambda x : x[1], reverse=True)\n",
    "\n",
    "print(*freqs_list_of_bigrams, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams model generator\n",
    "\n",
    "def bigram_generator(text):\n",
    "    current_token = text[-1]  #the last token in the text.\n",
    "    possible_bigrams = [x for x in bigrams if x[0] == current_token]  #to check if the last token of the given text is equal to the last token of the previous text generated by the code.\n",
    "    \n",
    "    if len(possible_bigrams) == 0:  # if there is no matched tokens (the last of the prev. the first of the next one) return as this;\n",
    "        return \"<eos>\"\n",
    "\n",
    "    sample_space = []\n",
    "\n",
    "    for i in possible_bigrams:\n",
    "        sample_space.append([i]*freqs_dicts_of_bigrams[i])\n",
    "    \n",
    "    sample_space = [x for X in sample_space for x in X]\n",
    "\n",
    "    next = rnd.choice(sample_space)\n",
    "\n",
    "    return next[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'i', 'am', 'going', 'to', 'take', 'a', 'book', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"<bos>\"]\n",
    "\n",
    "while True:\n",
    "    next_token = bigram_generator(sentence)\n",
    "    sentence.append(next_token)\n",
    "    if next_token == \"<eos>\":\n",
    "        break\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P( w3 | w1 , w2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'meet', 'my', 'friends', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'have', 'lunch', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'take', 'a', 'nap', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'watch', 'tv', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'play', 'video', 'games', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'go', 'for', 'a', 'walk', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'read', 'a', 'book', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'call', 'my', 'parents', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'write', 'an', 'email', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'clean', 'my', 'room', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'do', 'my', 'laundry', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'cook', 'dinner', '<eos>']\n",
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'go', 'to', 'bed', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = [\"<bos>\",\"<bos>\"] + [token.lower() for token in word_tokenize(sentence)] + [\"<eos>\"]\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "token_list = [x for X in tokenized_sentences for x in X]\n",
    "print(*tokenized_sentences, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<bos>', '<bos>', 'i'), 14)\n",
      "(('<bos>', 'i', 'am'), 14)\n",
      "(('i', 'am', 'going'), 14)\n",
      "(('am', 'going', 'to'), 14)\n",
      "(('going', 'to', 'go'), 3)\n",
      "(('to', 'go', 'to'), 2)\n",
      "(('going', 'to', 'meet'), 1)\n",
      "(('to', 'meet', 'my'), 1)\n",
      "(('meet', 'my', 'friends'), 1)\n",
      "(('my', 'friends', '<eos>'), 1)\n",
      "(('go', 'to', 'the'), 1)\n",
      "(('to', 'the', 'store'), 1)\n",
      "(('the', 'store', '<eos>'), 1)\n",
      "(('going', 'to', 'have'), 1)\n",
      "(('to', 'have', 'lunch'), 1)\n",
      "(('have', 'lunch', '<eos>'), 1)\n",
      "(('going', 'to', 'take'), 1)\n",
      "(('to', 'take', 'a'), 1)\n",
      "(('take', 'a', 'nap'), 1)\n",
      "(('a', 'nap', '<eos>'), 1)\n",
      "(('going', 'to', 'watch'), 1)\n",
      "(('to', 'watch', 'tv'), 1)\n",
      "(('watch', 'tv', '<eos>'), 1)\n",
      "(('going', 'to', 'play'), 1)\n",
      "(('to', 'play', 'video'), 1)\n",
      "(('play', 'video', 'games'), 1)\n",
      "(('video', 'games', '<eos>'), 1)\n",
      "(('to', 'go', 'for'), 1)\n",
      "(('go', 'for', 'a'), 1)\n",
      "(('for', 'a', 'walk'), 1)\n",
      "(('a', 'walk', '<eos>'), 1)\n",
      "(('going', 'to', 'read'), 1)\n",
      "(('to', 'read', 'a'), 1)\n",
      "(('read', 'a', 'book'), 1)\n",
      "(('a', 'book', '<eos>'), 1)\n",
      "(('going', 'to', 'call'), 1)\n",
      "(('to', 'call', 'my'), 1)\n",
      "(('call', 'my', 'parents'), 1)\n",
      "(('my', 'parents', '<eos>'), 1)\n",
      "(('going', 'to', 'write'), 1)\n",
      "(('to', 'write', 'an'), 1)\n",
      "(('write', 'an', 'email'), 1)\n",
      "(('an', 'email', '<eos>'), 1)\n",
      "(('going', 'to', 'clean'), 1)\n",
      "(('to', 'clean', 'my'), 1)\n",
      "(('clean', 'my', 'room'), 1)\n",
      "(('my', 'room', '<eos>'), 1)\n",
      "(('going', 'to', 'do'), 1)\n",
      "(('to', 'do', 'my'), 1)\n",
      "(('do', 'my', 'laundry'), 1)\n",
      "(('my', 'laundry', '<eos>'), 1)\n",
      "(('going', 'to', 'cook'), 1)\n",
      "(('to', 'cook', 'dinner'), 1)\n",
      "(('cook', 'dinner', '<eos>'), 1)\n",
      "(('go', 'to', 'bed'), 1)\n",
      "(('to', 'bed', '<eos>'), 1)\n"
     ]
    }
   ],
   "source": [
    "# 3-grams freq list\n",
    "\n",
    "trigrams = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for i in range(len(sentence)-2):\n",
    "        trigrams.append((sentence[i],sentence[i+1],sentence[i+2]))\n",
    "\n",
    "trigrams_freq_dict = {x:0 for x in trigrams}\n",
    "\n",
    "for i in trigrams:\n",
    "    trigrams_freq_dict[i] += 1\n",
    "\n",
    "trigrams_freq_list = [(x, trigrams_freq_dict[x]) for x in trigrams_freq_dict]\n",
    "trigrams_freq_list.sort(key = lambda x : x[1], reverse=True)\n",
    "\n",
    "print(*trigrams_freq_list, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-grams generator func\n",
    "\n",
    "def trigram_generator(text):\n",
    "    if len(text) < 2:\n",
    "        return \"<eos>\"\n",
    "\n",
    "    matched_token1 = text[-1]\n",
    "    matched_token2 = text[-2]\n",
    "    possible_trigrams = [x for x in trigrams if x[0] == matched_token2 and x[1] == matched_token1]\n",
    "\n",
    "    if len(possible_trigrams) == 0:\n",
    "        return \"<eos>\"\n",
    "\n",
    "    sample_space = []\n",
    "    for p in possible_trigrams:\n",
    "        sample_space.extend([p] * trigrams_freq_dict.get(p, 1))  \n",
    "\n",
    "    next_trigram = rnd.choice(sample_space)\n",
    "\n",
    "    return next_trigram[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '<bos>', 'i', 'am', 'going', 'to', 'meet', 'my', 'friends', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"<bos>\",\"<bos>\"]\n",
    "\n",
    "while True:\n",
    "    next_token = trigram_generator(sentence)\n",
    "    sentence.append(next_token)\n",
    "    if next_token == \"<eos>\":  \n",
    "        break\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
